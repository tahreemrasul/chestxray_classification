{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "79a6afc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import utils as ut\n",
    "import model_make as md\n",
    "import run\n",
    "import args\n",
    "\n",
    "import time\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79b94647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchxrayvision in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (0.0.37)\n",
      "Requirement already satisfied: torchvision>=0.5 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from torchxrayvision) (0.13.0)\n",
      "Requirement already satisfied: torch>=1 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from torchxrayvision) (1.12.0)\n",
      "Requirement already satisfied: requests>=1 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from torchxrayvision) (2.27.1)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from torchxrayvision) (9.0.1)\n",
      "Requirement already satisfied: scikit-image>=0.16 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from torchxrayvision) (0.19.2)\n",
      "Requirement already satisfied: pandas>=1 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from torchxrayvision) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from torchxrayvision) (1.21.5)\n",
      "Requirement already satisfied: tqdm>=4 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from torchxrayvision) (4.64.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas>=1->torchxrayvision) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas>=1->torchxrayvision) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1->torchxrayvision) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from requests>=1->torchxrayvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from requests>=1->torchxrayvision) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from requests>=1->torchxrayvision) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from requests>=1->torchxrayvision) (3.3)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from scikit-image>=0.16->torchxrayvision) (2.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from scikit-image>=0.16->torchxrayvision) (21.3)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from scikit-image>=0.16->torchxrayvision) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from scikit-image>=0.16->torchxrayvision) (2021.7.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from scikit-image>=0.16->torchxrayvision) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from scikit-image>=0.16->torchxrayvision) (1.7.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from packaging>=20.0->scikit-image>=0.16->torchxrayvision) (3.0.4)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from torch>=1->torchxrayvision) (4.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hamid\\anaconda3\\envs\\dl\\lib\\site-packages (from tqdm>=4->torchxrayvision) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchxrayvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "979dd3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = ut.dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5d1f3d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>LabelOne</th>\n",
       "      <th>LabelZero</th>\n",
       "      <th>LabelMulti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00143/study2/...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00140/study13...</td>\n",
       "      <td>[1, 0, 1, 0]</td>\n",
       "      <td>[1, 0, 1, 0]</td>\n",
       "      <td>[1, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00294/study14...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00314/study15...</td>\n",
       "      <td>[0, 1, 0, 1]</td>\n",
       "      <td>[0, 1, 0, 1]</td>\n",
       "      <td>[0, 1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00314/study9/...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00420/study2/...</td>\n",
       "      <td>[0, 1, 0, 1]</td>\n",
       "      <td>[0, 1, 0, 1]</td>\n",
       "      <td>[0, 1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00271/study2/...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00310/study25...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00210/study1/...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00337/study4/...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>869 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Path      LabelOne  \\\n",
       "0    CheXpert-v1.0-small/train/patient00143/study2/...  [0, 0, 0, 1]   \n",
       "1    CheXpert-v1.0-small/train/patient00140/study13...  [1, 0, 1, 0]   \n",
       "2    CheXpert-v1.0-small/train/patient00294/study14...  [0, 0, 0, 1]   \n",
       "3    CheXpert-v1.0-small/train/patient00314/study15...  [0, 1, 0, 1]   \n",
       "4    CheXpert-v1.0-small/train/patient00314/study9/...  [0, 1, 0, 0]   \n",
       "..                                                 ...           ...   \n",
       "864  CheXpert-v1.0-small/train/patient00420/study2/...  [0, 1, 0, 1]   \n",
       "865  CheXpert-v1.0-small/train/patient00271/study2/...  [0, 1, 0, 0]   \n",
       "866  CheXpert-v1.0-small/train/patient00310/study25...  [0, 1, 0, 0]   \n",
       "867  CheXpert-v1.0-small/train/patient00210/study1/...  [0, 1, 0, 0]   \n",
       "868  CheXpert-v1.0-small/train/patient00337/study4/...  [1, 0, 0, 0]   \n",
       "\n",
       "        LabelZero    LabelMulti  \n",
       "0    [0, 0, 0, 1]  [0, 0, 0, 1]  \n",
       "1    [1, 0, 1, 0]  [1, 0, 1, 0]  \n",
       "2    [0, 0, 0, 1]  [0, 0, 0, 1]  \n",
       "3    [0, 1, 0, 1]  [0, 1, 0, 1]  \n",
       "4    [0, 1, 0, 0]  [0, 1, 0, 0]  \n",
       "..            ...           ...  \n",
       "864  [0, 1, 0, 1]  [0, 1, 0, 1]  \n",
       "865  [0, 1, 0, 0]  [0, 1, 0, 0]  \n",
       "866  [0, 1, 0, 0]  [0, 1, 0, 0]  \n",
       "867  [0, 1, 0, 0]  [0, 1, 0, 0]  \n",
       "868  [1, 0, 0, 0]  [1, 0, 0, 0]  \n",
       "\n",
       "[869 rows x 4 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9037ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_df, val_df, test_df = ut.dataloader()\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    img_path = r\"E:\\Courses\"\n",
    "    train_dataset = ut.ChestXrayDataset(img_path,train_df, args.IMAGE_SIZE, True,'u-ones')\n",
    "    val_dataset = ut.ChestXrayDataset(img_path, val_df, args.IMAGE_SIZE, True,'u-ones')\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.BATCH_SIZE, shuffle=True, \n",
    "                                  num_workers=0, pin_memory=True)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.BATCH_SIZE, shuffle=False, \n",
    "                                num_workers=0, pin_memory=True)\n",
    "    \n",
    "    model = md.DenseNet121(num_classes=args.NUM_CLASSES).to(device)\n",
    "    \n",
    "    # Loss function\n",
    "    loss_criteria = nn.BCELoss()\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.LEARNING_RATE, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-5)\n",
    "\n",
    "    # Learning rate will be reduced automatically during training\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.LEARNING_RATE_SCHEDULE_FACTOR, \n",
    "                                                        patience=args.LEARNING_RATE_SCHEDULE_PATIENCE, \n",
    "                                                        mode='max', verbose=True)\n",
    "    \n",
    "    best_score = 0\n",
    "    model_path = \"densenet.pth\"\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    validation_score = []\n",
    "    validation_accuracy = []\n",
    "\n",
    "\n",
    "    # Config progress bar\n",
    "    mb = master_bar(range(5))\n",
    "    mb.names = ['Training loss', 'Validation loss', 'Validation AUROC']\n",
    "    x = []\n",
    "\n",
    "    nonimproved_epoch = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training each epoch\n",
    "    for epoch in mb:\n",
    "        mb.main_bar.comment = f'Best AUROC score: {best_score}'\n",
    "        x.append(epoch)\n",
    "\n",
    "        # Training\n",
    "        train_loss = run.epoch_training(epoch, model, train_dataloader, device, loss_criteria, optimizer, mb)\n",
    "        mb.write('Finish training epoch {} with loss {:.4f}'.format(epoch, train_loss))\n",
    "        training_losses.append(train_loss)\n",
    "\n",
    "        # Evaluating\n",
    "        val_loss, new_score, new_acc = run.evaluating(epoch, model, val_dataloader, device, loss_criteria, mb)\n",
    "        mb.write('Finish validation epoch {} with loss {:.4f}, AUROC score {:.4f} and accuracy {:.4f}'.format(epoch, \n",
    "                                                                                                              val_loss, \n",
    "                                                                                                              new_score,\n",
    "                                                                                                              new_acc))\n",
    "        validation_losses.append(val_loss)\n",
    "        validation_score.append(new_score)\n",
    "        validation_accuracy.append(new_acc)\n",
    "\n",
    "        # Update learning rate\n",
    "        lr_scheduler.step(new_score)\n",
    "\n",
    "        # Update training chart\n",
    "        mb.update_graph([[x, training_losses], [x, validation_losses], [x, validation_score]], [0,epoch+1], [0,1])\n",
    "\n",
    "        # Save model\n",
    "        if best_score < new_score:\n",
    "            mb.write(f\"Improve AUROC from {best_score} to {new_score}\")\n",
    "            best_score = new_score\n",
    "            nonimproved_epoch = 0\n",
    "            torch.save({\"model\": model.state_dict(), \n",
    "                        \"optimizer\": optimizer.state_dict(), \n",
    "                        \"best_score\": best_score, \n",
    "                        \"epoch\": epoch, \n",
    "                        \"lr_scheduler\": lr_scheduler.state_dict()}, model_path)\n",
    "        else: \n",
    "            nonimproved_epoch += 1\n",
    "        if nonimproved_epoch > 10:\n",
    "            break\n",
    "            print(\"Early stopping\")\n",
    "        if time.time() - start_time > 3600*8:\n",
    "            break\n",
    "            print(\"Out of time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c9e99437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting XRayResizer engine to cv2 could increase performance.\n",
      "Setting XRayResizer engine to cv2 could increase performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/5 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "img should be PIL Image. Got <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [73]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m x\u001b[38;5;241m.\u001b[39mappend(epoch)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_criteria\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m mb\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinish training epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m with loss \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, train_loss))\n\u001b[0;32m     52\u001b[0m training_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[1;32mE:\\Courses\\CheXpert-v1.0-small\\run.py:39\u001b[0m, in \u001b[0;36mepoch_training\u001b[1;34m(epoch, model, train_dataloader, device, loss_criteria, optimizer, mb)\u001b[0m\n\u001b[0;32m     36\u001b[0m training_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Storing sum of training losses\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# For each batch\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar(train_dataloader, parent\u001b[38;5;241m=\u001b[39mmb)):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Move X, Y  to device (GPU)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     42\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\fastprogress\\fastprogress.py:47\u001b[0m, in \u001b[0;36mProgressBar.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_interrupt()\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\fastprogress\\fastprogress.py:41\u001b[0m, in \u001b[0;36mProgressBar.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen):\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mE:\\Courses\\CheXpert-v1.0-small\\utils.py:173\u001b[0m, in \u001b[0;36mChestXrayDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    168\u001b[0m image_data \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m image_path,\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# Convert image to RGB channels\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# TODO: Image augmentation code would be placed here\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Resize and convert image to torch tensor\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m image_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_transformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_data, torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_labels[index])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torchvision\\transforms\\transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 94\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1584\u001b[0m, in \u001b[0;36mGrayscale.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m   1577\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   1579\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be converted to grayscale.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1582\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Grayscaled image.\u001b[39;00m\n\u001b[0;32m   1583\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrgb_to_grayscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_output_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_output_channels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torchvision\\transforms\\functional.py:1262\u001b[0m, in \u001b[0;36mrgb_to_grayscale\u001b[1;34m(img, num_output_channels)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     _log_api_usage_once(rgb_to_grayscale)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m-> 1262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_grayscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_output_channels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mrgb_to_grayscale(img, num_output_channels)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dl\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py:366\u001b[0m, in \u001b[0;36mto_grayscale\u001b[1;34m(img, num_output_channels)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39munused\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_grayscale\u001b[39m(img: Image\u001b[38;5;241m.\u001b[39mImage, num_output_channels: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pil_image(img):\n\u001b[1;32m--> 366\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_output_channels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    369\u001b[0m         img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[0.31629297 0.15592107 0.29681253 0.54696655]\n",
    " [0.3526758  0.1471397  0.23511569 0.6586677 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4c64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "incept = torchvision.models.inception_v3(weights='Inception_V3_Weights.DEFAULT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b6ab0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "incept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd63a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_features = incept.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ebba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d589a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "incept.fc = nn.Sequential(nn.Linear(model_output_features, 4),\n",
    "                                    nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9a969",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "incept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input dimension of last layer\n",
    "model_output_features = self.model.fc.in_features\n",
    "\n",
    "        # Replace last layer with new layer that have num_classes nodes, after that apply Sigmoid to the output\n",
    "self.model.fc = nn.Sequential(nn.Linear(model_output_features, num_classes),\n",
    "                                              nn.Sigmoid())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
